{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we will do throughout the exercise:\n",
    "We will write in details before any exercise. So just go ahead. <br />\n",
    "Almost every exercise is done (Graded and ungraded both) but surface plot of cost function and features. Till now, I have not understood the practical use of that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. WarmUpExercise.m:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.identity(5)             # Just kidding!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression with One Variable\n",
    "### 2.1 Plotting Data:\n",
    "Before starting any fitting model, it's really good to visualize data a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ex1data1.txt\n",
    "x,y=np.loadtxt('/home/suvranil/Dropbox/andrew_ng_python/ex1/ex1data1.txt',dtype= float,delimiter=',',skiprows=0,unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f98e3b0ff164f278acf00f07fb398ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting data: \n",
    "%matplotlib widget\n",
    "plt.scatter(x,y,label='node_50',color='r',marker='x',s=10)\n",
    "plt.xlabel('Population of city in 10,000s')\n",
    "plt.ylabel(' Profit in $10,000s')\n",
    "plt.title('Fig 1: Scatter plot of training data',)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Gradient Descent:\n",
    "Now before proceeding, just make one thing clear.We can easily do linear regression with one variable. But with multiple variables, it is too much time consuming. So we will try to vectorize the problem. There is a note matrix form of our known 'single' equation. Check it. <br />\n",
    "\n",
    "And **a little advice:** Before using any matrix equation, first check order of all matrices.\n",
    "\n",
    "## Matrix form of all important equation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig 02: Matrix form of all important equation](matrix_equation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important Data:m is number of traing example and n is number of features\n",
    "\n",
    "x_matrix=(np.matrix(np.vstack((np.ones(len(x)),x))))    # matrix of order (n+1)*m. Here n=1\n",
    "y_matrix=np.matrix(y)                                   # matrix of order 1*m\n",
    "initial_theta_matrix=np.matrix(np.zeros(x_matrix.shape[0])).T   # matrix of order (n+1)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Compute cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computecost(X,theta_matrix,Y): # check equation of cost function from notebook image\n",
    "    difference=np.dot(theta_matrix.T,X)-Y\n",
    "    cost=(1/(2*Y.shape[1]))*(np.dot(difference,difference.T))      # Y.shape[1] is number of training example\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32.07273388]]\n"
     ]
    }
   ],
   "source": [
    "# just checking whether cost function is working\n",
    "cost=computecost(x_matrix,initial_theta_matrix,y_matrix)     # calculate cost function for intial theta\n",
    "print(cost) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Gradient descent function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though we do use gradient descent for appropriate theta value only, we are storing cost function...\n",
    "# (after previous line) for seeing whether cost function is converging or not.\n",
    "def gradient_descent(learning_rate,iteration,theta_matrix,X,Y): # check equation of gradient descent from notebook image\n",
    "    cost_array=np.zeros(iteration)\n",
    "    for i in range(0,iteration,1):\n",
    "        difference=np.dot(theta_matrix.T,X)-Y\n",
    "        factor=learning_rate/Y.shape[1]            #Y.shape[1] is total number of training example\n",
    "        theta_matrix=theta_matrix-factor*np.matmul(X,difference.T)    \n",
    "        cost_array[i]=computecost(X,theta_matrix,Y)     # we will plot it to show that it converges with iteration\n",
    "    return cost_array,theta_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization\n",
    "alpha=0.01          #learning rate\n",
    "iterations=1500     # Number of iteration   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find appropriate theta_value\n",
    "cost,final_theta_matrix=gradient_descent(alpha,iterations,initial_theta_matrix,x_matrix,y_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cost function to ensure correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1b44145dd744d6ac8d6529cc1522ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot convergence of cost function\n",
    "%matplotlib widget\n",
    "plt.plot(cost,np.arange(0,iterations,1))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost function')\n",
    "plt.title('Fig 2: Convergence of Cost function')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Linear regression fit data with traing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_fit(theta_matrix,X):\n",
    "    return np.dot(theta_matrix.T,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eea9a906c4d4785b3817adb8b353dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training data with linear regression fit\n",
    "%matplotlib widget\n",
    "fit_data=linear_regression_fit(final_theta_matrix,x_matrix).tolist()\n",
    "plt.scatter(x,y,color='r',marker='x',s=10,label='Training examples')\n",
    "plt.scatter(x,fit_data,color='k',s=5,label='Linear regression')\n",
    "plt.xlabel('Population of city in 10,000s')\n",
    "plt.ylabel(' Profit in $10,000s')\n",
    "plt.title('Fig 3: Training data with Linear regression fit')\n",
    "plt.legend(edgecolor='black')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear regression with multiple variables\n",
    "### 3.1 Visualization of given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ex1data2.txt\n",
    "x_size,x_bedroom,y_price=np.loadtxt('/home/suvranil/Dropbox/andrew_ng_python/ex1/ex1data2.txt',dtype= float,delimiter=',',skiprows=0,unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743ed1c81efe42439f9e354d3bca6970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.scatter(x_size,y_price,label='size of the house',color='r',marker='x',s=10)\n",
    "plt.scatter(x_bedroom,y_price,label='number of bedrooms',color='g',marker='o',s=10)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Price of the house')\n",
    "plt.title('Fig 4: Visualization of of given data')\n",
    "plt.legend(edgecolor='black')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, we can easily understand that we need **Feature scaling** here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_size_new=(x_size-(np.mean(x_size)))/(np.std(x_size))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bedroom_new=(x_bedroom-(np.mean(x_bedroom)))/(np.std(x_bedroom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Visualization of data after feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cd43cb7ef34a4787ec1e97676b0357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(-3, 600000, 'value of any feature lies between 3 and -3')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.scatter(x_size_new,y_price,label='size of the house',color='r',marker='x',s=10)\n",
    "plt.scatter(x_bedroom_new,y_price,label='number of bedrooms',color='g',marker='o',s=10)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Price of the house')\n",
    "plt.title('Fig 4: Visualization of given data after feature scaling')\n",
    "plt.legend(edgecolor='black')\n",
    "plt.grid(True)\n",
    "plt.text(-3,600000,'value of any feature lies between 3 and -3', bbox=dict(facecolor='white', alpha=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_matrix=np.matrix([np.ones(len(x_size_new)),x_size_new,x_bedroom_new])\n",
    "y_matrix=np.matrix(y_price)\n",
    "initial_theta_matrix=np.matrix(np.zeros(x_matrix.shape[0])).T   # matrix of order (n+1)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Compute cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computecost(X,theta_matrix,Y):\n",
    "    difference=np.dot(theta_matrix.T,X)-Y\n",
    "    cost=(1/(2*Y.shape[1]))*(np.dot(difference,difference.T))      # Y.shape[1] is number of training example\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(learning_rate,iteration,theta_matrix,X,Y):\n",
    "    cost_array=np.zeros(iteration)\n",
    "    for i in range(0,iteration,1):\n",
    "        difference=np.dot(theta_matrix.T,X)-Y\n",
    "        factor=learning_rate/Y.shape[1]            #Y.shape[1] is total number of training example\n",
    "        theta_matrix=theta_matrix-factor*np.matmul(X,difference.T)    \n",
    "        cost_array[i]=computecost(X,theta_matrix,Y)     # we will plot it to show that it converges with iteration\n",
    "    return cost_array,theta_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization\n",
    "alpha=0.01\n",
    "iterations=10000      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find appropriate theta_value\n",
    "cost,final_theta_matrix=gradient_descent(alpha,iterations,initial_theta_matrix,x_matrix,y_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cost function to ensure correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c53f358d6bd4cec88c514c7895f00a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot convergence of cost function\n",
    "%matplotlib widget\n",
    "plt.plot(np.arange(0,iterations,1),cost)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost function')\n",
    "plt.title('Fig 5: Convergence of Cost function')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Selecting learning rate:\n",
    "It is pretty important to find a 'suitable' learning rate. So a just thumb rule is: Start from 0.001 and go ahead multiplying 3 and keep checking (For that plot cost function vs iteration plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=100\n",
    "alpha=[0.001,0.003,0.01]\n",
    "cost_box=np.matrix(np.zeros((len(alpha),iterations)))\n",
    "for i in range (0,len(alpha),1):\n",
    "    cost,theta=gradient_descent(alpha[i],iterations,initial_theta_matrix,x_matrix,y_matrix)\n",
    "    cost_box[i]= cost  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d69d442d464fc08e5fe685c4f74774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot convergence of cost function\n",
    "%matplotlib widget\n",
    "plt.plot(np.arange(0,iterations,1),np.ravel(cost_box[0]),label='alpha=0.001')\n",
    "plt.plot(np.arange(0,iterations,1),np.ravel(cost_box[1]),label='alpha=0.003')\n",
    "plt.plot(np.arange(0,iterations,1),np.ravel(cost_box[2]),label='alpha=0.01')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('cost function')\n",
    "plt.title('Fig 06: Finding a suitable learning rate')\n",
    "plt.legend(edgecolor='black')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cheers!!!! First hardle is done.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
